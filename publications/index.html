<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Bo  Dong | Publications</title>
<meta name="description" content="A beautiful, simple, clean, and responsive Jekyll theme for academics">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-447QQZBWVC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-447QQZBWVC');
  </script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=XXXXXXXXX"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'XXXXXXXXX' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Bo</span>   Dong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">List of recent publications</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR’21 Oral</abbr>
    
  
  </div>

  <div id="Dong_2021_CVPR" class="col-sm-8">
    
      <div class="title">Depth-Aware Mirror Segmentation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Mei, Haiyang,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Dong, Wen,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cs.wm.edu/~ppeers/" target="_blank">Peers, Pieter</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Xin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Qiang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wei, Xiaopeng
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Mei_Depth-Aware_Mirror_Segmentation_CVPR_2021_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/2021/CVPR2021_RGBD.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a novel mirror segmentation method that leverages depth estimates from ToF-based cameras as an additional cue to disambiguate challenging cases where the contrast or relation in RGB colors between the mirror reflection and the surrounding scene is subtle. A key observation is that ToF depth estimates do not report the true depth of the mirror surface, but instead return the total length of the reflected light paths, thereby creating obvious depth discontinuities at the mirror boundaries. To exploit depth information in mirror segmentation, we first construct a large-scale RGB-D mirror segmentation dataset, which we subsequently employ to train a novel depth-aware mirror segmentation framework. Our mirror segmentation framework first locates the mirrors based on color and depth discontinuities and correlations. Next, our model further refines the mirror boundaries through contextual contrast taking into accountboth color and depth information. We extensively validate our depth-aware mirror segmentation method and demonstrate that our model outperforms state-of-the-art RGB and RGB-D based methods for mirror segmentation. Experimental results also show that depth is a powerful cue for mirror segmentation.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MobiSys’20</abbr>
    
  
  </div>

  <div id="10.1145/3386901.3388917" class="col-sm-8">
    
      <div class="title">EMO: Real-Time Emotion Recognition from Single-Eye Images for Resource-Constrained Eyewear Devices</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Wu, Hao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Feng, Jinghao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tian, Xuejin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sun, Edward,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Yunxin,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://cs.nju.edu.cn/fxu/" target="_blank">Xu, Fengyuan</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zhong, Sheng
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://dl.acm.org/doi/abs/10.1145/3386901.3388917" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/2020/Wu_2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Real-time user emotion recognition is highly desirable for many applications on eyewear devices like smart glasses. However, it is very challenging to enable this capability on such devices due to tightly constrained image contents (only eye-area images available from the on-device eye-tracking camera) and computing resources of the embedded system. In this paper, we propose and develop a novel system called EMO that can recognize, on top of a resource-limited eyewear device, real-time emotions of the user who wears it. Unlike most existing solutions that require whole-face images to recognize emotions, EMO only utilizes the single-eye-area images captured by the eye-tracking camera of the eyewear. To achieve this, we design a customized deep-learning network to effectively extract emotional features from input single-eye images and a personalized feature classifier to accurately identify a user’s emotions. EMO also exploits the temporal locality and feature similarity among consecutive video frames of the eye-tracking camera to further reduce the recognition latency and system resource usage. We implement EMO on two hardware platforms and conduct comprehensive experimental evaluations. Our results demonstrate that EMO can continuously recognize seven-type emotions at 12.8 frames per second with a mean accuracy of 72.2%, significantly outperforming the state-of-the-art approach, and consume much fewer system resources.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISBI’19</abbr>
    
  
  </div>

  <div id="8759428" class="col-sm-8">
    
      <div class="title">XAI-CBIR: Explainable AI System for Content based Retrieval of Video Frames from Minimally Invasive Surgery Videos</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Chittajallu, D. R.,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tunison, P.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Collins, R.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wells, K.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fleshman, J.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sankaranarayanan, G.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schwaitzberg, S.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cavuoto, L.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Enquobahrie, A.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we present a human-in-the-loop explainable AI (XAI) system for content based image retrieval (CBIR) of video frames similar to a query image from minimally invasive surgery (MIS) videos for surgical education. It extracts semantic descriptors from MIS video frames using a self-supervised deep learning model. It then employs an iterative query refinement strategy where in a binary classifier trained online based on relevance feedback from the user is used to iteratively refine the search results. Lastly, it uses an XAI technique to generate a saliency map that provides a visual explanation of why the system considers a retrieved image to be similar to the query image. We evaluated the proposed XAI-CBIR system on the public Cholec80 dataset containing 80 videos of minimally invasive cholecystectomy surgeries with encouraging results.
	    </p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPRW’19</abbr>
    
  
  </div>

  <div id="dong2019explainability" class="col-sm-8">
    
      <div class="title">Explainability for Content-Based Image Retrieval.</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Collins, Roddy,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hoogs, Anthony
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In CVPR Workshops</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Explainable%20AI/Dong_Explainability_for_Content-Based_Image_Retrieval_CVPRW_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/2019/Dong_CVPRW_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECCV’18 Demo</abbr>
    
  
  </div>

  <div id="dong2018ECCVDemo" class="col-sm-8">
    
      <div class="title">Iterative Query Refinement with Visual Explanations.</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Vitali, Petsiuk,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Das, Abir,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Saenko, Kate,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Collins, Roddy,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hoogs, Anthony
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ECCV Demo</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://eccv2018.org/program/demo-sessions/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    
      <a href="https://youtu.be/WTnWofS1nEE" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="DBLP:journals/corr/abs-1806-01477" class="col-sm-8">
    
      <div class="title">An Explainable Adversarial Robustness Metric for Deep Learning Neural
               Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Agarwal, Chirag,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Schonfeld, Dan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hoogs, Anthony
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>CoRR</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
    
      <a href="https://arxiv.org/abs/1806.01477" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACNS’15</abbr>
    
  
  </div>

  <div id="10.1007/978-3-319-28166-7_22" class="col-sm-8">
    
      <div class="title">Exploiting Eye Tracking for Smartphone Authentication</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Dachuan,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gao, Xing,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wang, Haining
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Applied Cryptography and Network Security</em>
      
      
        2015
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://link.springer.com/chapter/10.1007/978-3-319-28166-7_22" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/2015/Liu2015.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Traditional user authentication methods using passcode or finger movement on smartphones are vulnerable to shoulder surfing attack, smudge attack, and keylogger attack. These attacks are able to infer a passcode based on the information collection of user’s finger movement or tapping input. As an alternative user authentication approach, eye tracking can reduce the risk of suffering those attacks effectively because no hand input is required. However, most existing eye tracking techniques are designed for large screen devices. Many of them depend on special hardware like high resolution eye tracker and special process like calibration, which are not readily available for smartphone users. In this paper, we propose a new eye tracking method for user authentication on a smartphone. It utilizes the smartphone’s front camera to capture a user’s eye movement trajectories which are used as the input of user authentication. No special hardware or calibration process is needed. We develop a prototype and evaluate its effectiveness on an Android smartphone. We recruit a group of volunteers to participate in the user study. Our evaluation results show that the proposed eye tracking technique achieves very high accuracy in user authentication.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SIGGRAPH’15</abbr>
    
  
  </div>

  <div id="10.1145/2766979" class="col-sm-8">
    
      <div class="title">Measurement-Based Editing of Diffuse Albedo with Consistent Interreflections</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://yuedong.shading.me/" target="_blank">Dong, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.xtong.info/" target="_blank">Tong, Xin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cs.wm.edu/~ppeers/" target="_blank">Peers, Pieter</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ACM Trans. Graph.</em>
      
      
        2015
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/2015/10.1.1.711.9735.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://www.youtube.com/watch?v=01hdbbag63w&amp;ab_channel=BoDong" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present a novel measurement-based method for editing the albedo of diffuse surfaces with consistent interreflections in a photograph of a scene under natural lighting. Key to our method is a novel technique for decomposing a photograph of a scene in several images that encode how much of the observed radiance has interacted a specified number of times with the target diffuse surface. Altering the albedo of the target area is then simply a weighted sum of the decomposed components. We estimate the interaction components by recursively applying the light transport operator and formulate the resulting radiance in each recursion as a linear expression in terms of the relevant interaction components. Our method only requires a camera-projector pair, and the number of required measurements per scene is linearly proportional to the decomposition degree for a single target area. Our method does not impose restrictions on the lighting or on the material properties in the unaltered part of the scene. Furthermore, we extend our method to accommodate editing of the albedo in multiple target areas with consistent interreflections and we introduce a prediction model for reducing the acquisition cost. We demonstrate our method on a variety of scenes and validate the accuracy on both synthetic and real examples.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR’14</abbr>
    
  
  </div>

  <div id="Dong_2014_CVPR" class="col-sm-8">
    
      <div class="title">Scattering Parameters and Surface Normals from Homogeneous Translucent Materials using Photometric Stereo</div>
      <div class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                <em>Dong, Bo</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Moore, Kathleen D.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Weiyi,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cs.wm.edu/~ppeers/" target="_blank">Peers, Pieter</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2014
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openaccess.thecvf.com/content_cvpr_2014/html/Dong_Scattering_Parameters_and_2014_CVPR_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/2014/Dong_2014_CVPR_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://www.youtube.com/watch?v=U4XMxs9faQ8&amp;ab_channel=BoDong" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper proposes a novel photometric stereo solution to jointly estimate surface normals and scattering parameters from a globally planar, homogeneous, translucent object. Similar to classic photometric stereo, our method only requires as few as three observations of the translucent object under directional lighting. Naively applying classic photometric stereo results in blurred photometric normals. We develop a novel blind deconvolution algorithm based on inverse rendering for recovering the sharp surface normals and the material properties. We demonstrate our method on a variety of translucent objects.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Bo  Dong.
    
    
    
    Last updated: June 14, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
